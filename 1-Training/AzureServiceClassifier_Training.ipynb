{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Training Tensorflow 2.0 Model on Azure Machine Learning Service\n",
    "\n",
    "## Overview of the part 1\n",
    "This notebook is Part 1 (Preparing Data and Model Training) of a four part workshop that demonstrates an end-to-end workflow using Tensorflow 2.0 on Azure Machine Learning service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Preparing Data and Model Training](https://github.com/microsoft/bert-stack-overflow/blob/master/1-Training/AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/microsoft/bert-stack-overflow/blob/master/2-Inferencing/AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/microsoft/bert-stack-overflow/tree/master/3-ML-Ops)\n",
    "- Part 4: [Explaining Your Model Interpretability](https://github.com/microsoft/bert-stack-overflow/blob/master/4-Interpretibility/IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "**This notebook will cover the following topics:**\n",
    "\n",
    "- Stackoverflow question tagging problem\n",
    "- Introduction to Transformer and BERT deep learning models\n",
    "- Introduction to Azure Machine Learning service\n",
    "- Preparing raw data for training using Apache Spark\n",
    "- Registering cleaned up training data as a Dataset\n",
    "- Debugging the model in Tensorflow 2.0 Eager Mode\n",
    "- Training the model on GPU cluster\n",
    "- Monitoring training progress with built-in Tensorboard dashboard \n",
    "- Automated search of best hyper-parameters of the model\n",
    "- Registering the trained model for future deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook is designed to be run in Azure ML Notebook VM. See [readme](https://github.com/microsoft/bert-stack-overflow/blob/master/README.md) file for instructions on how to create Notebook VM and open this notebook in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Azure Machine Learning Python SDK version\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. Let's check the version of the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow Question Tagging Problem \n",
    "In this workshop we will use powerful language understanding model to automatically route Stackoverflow questions to the appropriate support team on the example of Azure services.\n",
    "\n",
    "One of the key tasks to ensuring long term success of any Azure service is actively responding to related posts in online forums such as Stackoverflow. In order to keep track of these posts, Microsoft relies on the associated tags to direct questions to the appropriate support team. While Stackoverflow has different tags for each Azure service (azure-web-app-service, azure-virtual-machine-service, etc), people often use the generic **azure** tag. This makes it hard for specific teams to track down issues related to their product and as a result, many questions get left unanswered. \n",
    "\n",
    "**In order to solve this problem, we will build a model to classify posts on Stackoverflow with the appropriate Azure service tag.**\n",
    "\n",
    "We will be using a BERT (Bidirectional Encoder Representations from Transformers) model which was published by researchers at Google AI Reasearch. Unlike prior language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of natural language processing (NLP) tasks without substantial architecture modifications.\n",
    "\n",
    "## Why use BERT model?\n",
    "[Introduction of BERT model](https://arxiv.org/pdf/1810.04805.pdf) changed the world of NLP. Many NLP problems that before relied on specialized models to achive state of the art performance are now solved with BERT better and with more generic approach.\n",
    "\n",
    "If we look at the leaderboards on such popular NLP problems as GLUE and SQUAD, most of the top models are based on BERT:\n",
    "* [GLUE Benchmark Leaderboard](https://gluebenchmark.com/leaderboard/)\n",
    "* [SQuAD Benchmark Leaderboard](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "\n",
    "Recently, Allen Institue for AI announced new language understanding system called Aristo [https://allenai.org/aristo/](https://allenai.org/aristo/). The system has been developed for 20 years, but it's performance was stuck at 60% on 8th grade science test. The result jumped to 90% once researchers adopted BERT as core language understanding component. With BERT Aristo now solves the test with A grade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview of How BERT model works\n",
    "\n",
    "The foundation of BERT model is Transformer model, which was introduced in [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762). Before that event the dominant way of processing language was Recurrent Neural Networks (RNNs). Let's start our overview with RNNs.\n",
    "\n",
    "## RNNs\n",
    "\n",
    "RNNs were powerful way of processing language due to their ability to memorize its previous state and perform sophisticated inference based on that.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/400/1*L38xfe59H5tAgvuIjKoWPg.png\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "_Taken from [1](https://towardsdatascience.com/transformers-141e32e69591)_\n",
    "\n",
    "Applied to language translation task, the processing dynamics looked like this.\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*8GcdjBU5TAP36itWBcZ6iA.gif)\n",
    "_Taken from [2](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)_\n",
    "    \n",
    "But RNNs suffered from 2 disadvantes:\n",
    "1. Sequential computation put a limit on parallelization, which limited effectiveness of larger models.\n",
    "2. Long term relationships between words were harder to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers were designed to address these two limitations of RNNs.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2436/1*V2435M1u0tiSOz4nRBfl4g.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "_Taken from [3](http://jalammar.github.io/illustrated-transformer/)_\n",
    "\n",
    "In each Encoder layer Transformer performs Self-Attention operation which detects relationships between all word embeddings in one matrix multiplication operation. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2176/1*fL8arkEFVKA3_A7VBgapKA.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "_Taken from [4](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model\n",
    "\n",
    "BERT is a very large network with multiple layers of Transformers (12 for BERT-base, and 24 for BERT-large). The model is first pre-trained on large corpus of text data (WikiPedia + books) using un-superwised training (predicting masked words in a sentence). During pre-training the model absorbs significant level of language understanding.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-output-vector.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "_Taken from [5](http://jalammar.github.io/illustrated-bert/)_\n",
    "\n",
    "Pre-trained network then can easily be fine-tuned to solve specific language task, like answering questions, or categorizing spam emails.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-classifier.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "_Taken from [5](http://jalammar.github.io/illustrated-bert/)_\n",
    "\n",
    "The end-to-end training process of the stackoverflow question tagging model looks like this:\n",
    "\n",
    "![](images/model-training-e2e.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n",
    "\n",
    "\n",
    "#### How can we use it for training machine learning models?\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload.\n",
    "\n",
    "To facilitate training, the Azure Machine Learning Python SDK provides a high-level abstraction, the estimator class, which allows users to easily train their models in the Azure ecosystem. You can create and use an Estimator object to submit any training code you want to run on remote compute, whether it's a single-node run or distributed training across a GPU cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace holds all your experiments, compute targets, models, datastores, etc.\n",
    "\n",
    "You can [open ml.azure.com](https://ml.azure.com) to access your workspace resources through a graphical user interface of **Azure Machine Learning studio**.\n",
    "\n",
    "![](./images/aml-workspace.png)\n",
    "\n",
    "**You will be asked to login in the next step. Use your Microsoft AAD credentials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: tf-world\n",
      "Azure region: eastus\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: tf-world\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments. \n",
    "\n",
    "For this tutorial, we will create an auto-scaling [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute?view=azure-ml-py) cluster, which is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. To create the cluster, we need to specify the following parameters:\n",
    "\n",
    "- `vm_size`: The is the type of GPUs that we want to use in our cluster. For this tutorial, we will use **Standard_NC12s_v3 (NVIDIA V100) GPU Machines** .\n",
    "- `idle_seconds_before_scaledown`: This is the number of seconds before a node will scale down in our auto-scaling cluster. We will set this to **6000** seconds. \n",
    "- `min_nodes`: This is the minimum numbers of nodes that the cluster will have. To avoid paying for compute while they are not being used, we will set this to **0** nodes.\n",
    "- `max_modes`: This is the maximum number of nodes that the cluster will scale up to. Will will set this to **2** nodes.\n",
    "\n",
    "**When jobs are submitted to the cluster it takes approximately 5 minutes to allocate new nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = 'v100cluster'\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC12s_v3', \n",
    "                                                       idle_seconds_before_scaledown=6000,\n",
    "                                                       min_nodes=0, \n",
    "                                                       max_nodes=2)\n",
    "\n",
    "compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our compute target was created successfully, we can check it's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_target.get_status().serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the compute target has already been created, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_target = workspace.compute_targets['v100cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Using Apache Spark\n",
    "\n",
    "To train our model, we used the Stackoverflow data dump from [Stack exchange archive](https://archive.org/download/stackexchange). Since the Stackoverflow _posts_ dataset is 12GB, we prepared the data using [Apache Spark](https://spark.apache.org/) framework on a scalable Spark compute cluster in [Azure Databricks](https://azure.microsoft.com/en-us/services/databricks/). \n",
    "\n",
    "For the purpose of this tutorial, we have processed the data ahead of time and uploaded it to an [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. The full data processing notebook can be found in the _spark_ folder.\n",
    "\n",
    "* **ACTION**: Open and explore [data preparation notebook](spark/stackoverflow-data-prep.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is used to store connection information to a central data storage. This allows you to access your storage without having to hard code this (potentially confidential) information into your scripts. \n",
    "\n",
    "In this tutorial, the data was been previously prepped and uploaded into a central [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. We will register this container into our workspace as a datastore using a [shared access signature (SAS) token](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "datastore_name = 'tfworld'\n",
    "container_name = 'azureml-blobstore-7c6bdd88-21fa-453a-9c80-16998f02935f'\n",
    "account_name = 'tfworld6818510241'\n",
    "sas_token = '?sv=2019-02-02&ss=bfqt&srt=sco&sp=rl&se=2020-06-01T14:18:31Z&st=2019-11-05T07:18:31Z&spr=https&sig=Z4JmM0V%2FQzoFNlWS3a3vJxoGAx58iCz2HAWtmeLDbGE%3D'\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(workspace=workspace, \n",
    "                                                    datastore_name=datastore_name, \n",
    "                                                    container_name=container_name,\n",
    "                                                    account_name=account_name, \n",
    "                                                    sas_token=sas_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datastore = workspace.datastores['tfworld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if my data wasn't already hosted remotely?\n",
    "All workspaces also come with a blob container which is registered as a default datastore. This allows you to easily upload your own data to a remote storage location. You can access this datastore and upload files as follows:\n",
    "```\n",
    "datastore = workspace.get_default_datastore()\n",
    "ds.upload(src_dir='<LOCAL-PATH>', target_path='<REMOTE-PATH>')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "Azure Machine Learning service supports first class notion of a Dataset. A [Dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py) is a resource for exploring, transforming and managing data in Azure Machine Learning. The following Dataset types are supported:\n",
    "\n",
    "* [TabularDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) represents data in a tabular format created by parsing the provided file or list of files.\n",
    "\n",
    "* [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) references single or multiple files in datastores or from public URLs.\n",
    "\n",
    "First, we will use visual tools in Azure ML studio to register and explore our dataset as Tabular Dataset.\n",
    "\n",
    "* **ACTION**: Follow [create-dataset](images/create-dataset.ipynb) guide to create Tabular Dataset from our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use created dataset in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Get a dataset by name\n",
    "tabular_ds = Dataset.get_by_name(workspace=workspace, name='Stackoverflow dataset')\n",
    "\n",
    "# Load a TabularDataset into pandas DataFrame\n",
    "df = tabular_ds.to_pandas_dataframe()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset using SDK\n",
    "\n",
    "In addition to UI we can register datasets using SDK. In this workshop we will register second type of Datasets using code - File Dataset. File Dataset allows specific folder in our datastore that contains our data files to be registered as a Dataset.\n",
    "\n",
    "There is a folder within our datastore called **azure-service-data** that contains all our training and testing data. We will register this as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azure_dataset = Dataset.File.from_files(path=(datastore, 'azure-service-classifier/data'))\n",
    "\n",
    "azure_dataset = azure_dataset.register(workspace=workspace,\n",
    "                                       name='Azure Services Dataset',\n",
    "                                       description='Dataset containing azure related posts on Stackoverflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azure_dataset = workspace.datasets['Azure Services Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop the training code is provided in [train.py](./train.py) and [model.py](./model.py) files. The model is based on popular [huggingface/transformers](https://github.com/huggingface/transformers) libary. Transformers library provides performant implementation of BERT model with high level and easy to use APIs based on Tensorflow 2.0.\n",
    "\n",
    "![](https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png)\n",
    "\n",
    "*  **ACTION**: Explore _train.py_ and _model.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)\n",
    "* NOTE: You can also explore the files using Jupyter or Jupyter Lab UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally\n",
    "\n",
    "Let's try running the script locally to make sure it works before scaling up to use our compute cluster. To do so, you will need to install the transformers libary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken a small partition of the dataset and included it in this repository. Let's take a quick look at the format of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "data = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what the data looks like, let's test out our script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} train.py --data_dir {data_dir} --max_seq_length 128 --batch_size 16 --learning_rate 3e-5 --steps_per_epoch 5 --num_epochs 1 --export_dir ../outputs/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging in TensorFlow 2.0 Eager Mode\n",
    "\n",
    "Eager mode is new feature in TensorFlow 2.0 which makes understanding and debugging models easy. Let's start by configuring our remote debugging environment.\n",
    "\n",
    "#### Configure VS Code Remote connection to Notebook VM\n",
    "\n",
    "* **ACTION**: Install [Microsoft VS Code](https://code.visualstudio.com/) on your local machine.\n",
    "\n",
    "* **ACTION**: Follow this [configuration guide](https://github.com/danielsc/azureml-debug-training/blob/master/Setting%20up%20VSCode%20Remote%20on%20an%20AzureML%20Notebook%20VM.md) to setup VS Code Remote connection to Notebook VM.\n",
    "\n",
    "#### Debug training code using step-by-step debugger\n",
    "\n",
    "* **ACTION**: Open Remote VS Code session to your Notebook VM.\n",
    "* **ACTION**: Open file `/home/azureuser/cloudfiles/code/<username>/bert-stack-overflow/1-Training/train_eager.py`.\n",
    "* **ACTION**: Set break point in the file and start Python debugging session. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a CPU machine training on a full dataset will take approximatly 1.5 hours. Although it's a small dataset, it still takes a long time. Let's see how we can speed up the training by using latest NVidia V100 GPUs in the Azure cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our compute target, dataset, and training script working locally, it is time to scale up so that the script can run faster. We will start by creating an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'azure-service-classifier' \n",
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TensorFlow Estimator\n",
    "\n",
    "The Azure Machine Learning Python SDK Estimator classes allow you to easily construct run configurations for your experiments. They allow you too define parameters such as the training script to run, the compute target to run it on, framework versions, additional package requirements, etc. \n",
    "\n",
    "You can also use a generic [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) to submit training scripts that use any learning framework you choose.\n",
    "\n",
    "For popular libaries like PyTorch and Tensorflow you can use their framework specific estimators. We will use the [TensorFlow Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "estimator1 = TensorFlow(source_directory='.',\n",
    "                        entry_script='train.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/model'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick description for each of the parameters we have just defined:\n",
    "\n",
    "- `source_directory`: This specifies the root directory of our source code. \n",
    "- `entry_script`: This specifies the training script to run. It should be relative to the source_directory.\n",
    "- `compute_target`: This specifies to compute target to run the job on. We will use the one created earlier.\n",
    "- `script_params`: This specifies the input parameters to the training script. Please note:\n",
    "\n",
    "    1) *azure_dataset.as_named_input('azureservicedata').as_mount()* mounts the dataset to the remote compute and provides the path to the dataset on our datastore. \n",
    "    \n",
    "    2) All outputs from the training script must be outputted to an './outputs' directory as this is the only directory that will be saved to the run. \n",
    "    \n",
    "    \n",
    "- `framework_version`: This specifies the version of TensorFlow to use. Use Tensorflow.get_supported_verions() to see all supported versions.\n",
    "- `use_gpu`: This will use the GPU on the compute target for training if set to True.\n",
    "- `pip_packages`: This allows you to define any additional libraries to install before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Submit First Run \n",
    "\n",
    "We can now train our model by submitting the estimator object as a [run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run1 = experiment.submit(estimator1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the current status of the run and stream the logs from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b9771e1b5241a3b547462ee3ae9484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Preparing\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/azure-service-classifier/runs/azure-service-classifier_1572941031_afae9b73?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/tf-world/workspaces/tf-world\", \"run_id\": \"azure-service-classifier_1572941031_afae9b73\", \"run_properties\": {\"run_id\": \"azure-service-classifier_1572941031_afae9b73\", \"created_utc\": \"2019-11-05T08:03:56.674875Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"batchai\", \"ContentSnapshotId\": \"bb3af30c-764f-4f25-85e2-857df51ab757\", \"azureml.git.repository_uri\": \"https://github.com/microsoft/bert-stack-overflow\", \"mlflow.source.git.repoURL\": \"https://github.com/microsoft/bert-stack-overflow\", \"azureml.git.branch\": \"john\", \"mlflow.source.git.branch\": \"john\", \"azureml.git.commit\": \"69016d7ea04406fb4bd3c21889a189939db13f5c\", \"mlflow.source.git.commit\": \"69016d7ea04406fb4bd3c21889a189939db13f5c\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_8b8765b28a4a858399aa44e9096a8d04\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Preparing\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://tfworld6818510241.blob.core.windows.net/azureml/ExperimentRun/dcid.azure-service-classifier_1572941031_afae9b73/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=OH01XkDPZBeS14EeGdBdjl%2Bl8oApXY%2FQ4MxHmnDsXm8%3D&st=2019-11-05T08%3A03%3A39Z&se=2019-11-05T16%3A13%3A39Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/20_image_build_log.txt\"]], \"run_duration\": \"0:09:42\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2019/11/05 08:04:08 Downloading source code...\\r\\n2019/11/05 08:04:11 Finished downloading source code\\r\\n2019/11/05 08:04:11 Creating Docker network: acb_default_network, driver: 'bridge'\\n2019/11/05 08:04:12 Successfully set up Docker network: acb_default_network\\n2019/11/05 08:04:12 Setting up Docker configuration...\\n2019/11/05 08:04:13 Successfully set up Docker configuration\\n2019/11/05 08:04:13 Logging in to registry: tfworld0b69c845.azurecr.io\\n2019/11/05 08:04:14 Successfully logged into tfworld0b69c845.azurecr.io\\r\\n2019/11/05 08:04:14 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2019/11/05 08:04:14 Scanning for dependencies...\\n2019/11/05 08:04:15 Successfully scanned dependencies\\n2019/11/05 08:04:15 Launching container with name: acb_step_0\\nSending build context to Docker daemon  59.39kB\\r\\r\\nStep 1/14 : FROM mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04@sha256:887cd45a30688c445afa08378f88b4e5aef820bc3e2c9af6e1c89a41a8aaf33e\\nsha256:887cd45a30688c445afa08378f88b4e5aef820bc3e2c9af6e1c89a41a8aaf33e: Pulling from azureml/base-gpu\\n35c102085707: Pulling fs layer\\n251f5509d51d: Pulling fs layer\\n8e829fe70a46: Pulling fs layer\\n6001e1789921: Pulling fs layer\\n9f0a21d58e5d: Pulling fs layer\\n8810fcda1e6e: Pulling fs layer\\nd701a76e3193: Pulling fs layer\\n34be232fb7a6: Pulling fs layer\\n7e62b1ed3410: Pulling fs layer\\n47526c6630b9: Pulling fs layer\\nc8225a1ab66d: Pulling fs layer\\nba6ff8ef019c: Pulling fs layer\\n8c8fdd713d59: Pulling fs layer\\n412b4d6d8136: Pulling fs layer\\n517038e68f47: Pulling fs layer\\n10001740b54b: Pulling fs layer\\n49e1d575d70f: Pulling fs layer\\n6001e1789921: Waiting\\n9f0a21d58e5d: Waiting\\n8810fcda1e6e: Waiting\\nd701a76e3193: Waiting\\n34be232fb7a6: Waiting\\n7e62b1ed3410: Waiting\\n47526c6630b9: Waiting\\nc8225a1ab66d: Waiting\\nba6ff8ef019c: Waiting\\n8c8fdd713d59: Waiting\\n412b4d6d8136: Waiting\\n517038e68f47: Waiting\\n10001740b54b: Waiting\\n49e1d575d70f: Waiting\\n8e829fe70a46: Verifying Checksum\\n8e829fe70a46: Download complete\\r\\n251f5509d51d: Verifying Checksum\\n251f5509d51d: Download complete\\n6001e1789921: Verifying Checksum\\n6001e1789921: Download complete\\n35c102085707: Verifying Checksum\\n35c102085707: Download complete\\nd701a76e3193: Verifying Checksum\\nd701a76e3193: Download complete\\n8810fcda1e6e: Verifying Checksum\\n8810fcda1e6e: Download complete\\n9f0a21d58e5d: Verifying Checksum\\n9f0a21d58e5d: Download complete\\n35c102085707: Pull complete\\r\\n251f5509d51d: Pull complete\\n8e829fe70a46: Pull complete\\n6001e1789921: Pull complete\\n9f0a21d58e5d: Pull complete\\r\\n47526c6630b9: Verifying Checksum\\n47526c6630b9: Download complete\\r\\n8810fcda1e6e: Pull complete\\n34be232fb7a6: Verifying Checksum\\n34be232fb7a6: Download complete\\nc8225a1ab66d: Verifying Checksum\\nc8225a1ab66d: Download complete\\n8c8fdd713d59: Verifying Checksum\\nd701a76e3193: Pull complete\\n8c8fdd713d59: Download complete\\r\\nba6ff8ef019c: Verifying Checksum\\nba6ff8ef019c: Download complete\\n517038e68f47: Verifying Checksum\\n517038e68f47: Download complete\\n412b4d6d8136: Verifying Checksum\\n412b4d6d8136: Download complete\\n49e1d575d70f: Verifying Checksum\\n49e1d575d70f: Download complete\\r\\n10001740b54b: Verifying Checksum\\n10001740b54b: Download complete\\n7e62b1ed3410: Verifying Checksum\\n7e62b1ed3410: Download complete\\r\\n34be232fb7a6: Pull complete\\r\\n7e62b1ed3410: Pull complete\\r\\n47526c6630b9: Pull complete\\r\\nc8225a1ab66d: Pull complete\\r\\nba6ff8ef019c: Pull complete\\r\\n8c8fdd713d59: Pull complete\\n412b4d6d8136: Pull complete\\r\\n517038e68f47: Pull complete\\n10001740b54b: Pull complete\\n49e1d575d70f: Pull complete\\nDigest: sha256:887cd45a30688c445afa08378f88b4e5aef820bc3e2c9af6e1c89a41a8aaf33e\\nStatus: Downloaded newer image for mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04@sha256:887cd45a30688c445afa08378f88b4e5aef820bc3e2c9af6e1c89a41a8aaf33e\\n ---> caf09b9e3e4d\\nStep 2/14 : USER root\\n ---> Running in 783fc5a92066\\r\\nRemoving intermediate container 783fc5a92066\\n ---> 6ed98cded985\\nStep 3/14 : RUN mkdir -p $HOME/.cache\\n ---> Running in 76a5af8c2f60\\r\\nRemoving intermediate container 76a5af8c2f60\\n ---> 833c7485e88c\\nStep 4/14 : WORKDIR /\\n ---> Running in 28f408902b12\\nRemoving intermediate container 28f408902b12\\n ---> 6fa6f0d06eb3\\nStep 5/14 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\\r\\n ---> 49ac6f789d08\\nStep 6/14 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\\n ---> Running in 3c8e37aeb68d\\nRemoving intermediate container 3c8e37aeb68d\\n ---> a542c833745e\\nStep 7/14 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\\r\\n ---> a24c7198d716\\nStep 8/14 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \\\"$HOME/.cache/pip\\\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \\\"$CONDA_ROOT_DIR/pkgs\\\" && find \\\"$CONDA_ROOT_DIR\\\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\\n ---> Running in 16ad378da3e0\\nSolving environment: ...working... done\\r\\n\\u001b[91m\\n\\n==> WARNING: A newer version of conda exists. <==\\n  current version: 4.5.11\\n  latest version: 4.7.12\\n\\nPlease update conda by running\\n\\n    $ conda update -n base -c defaults conda\\n\\n\\n\\rreadline-6.2         | 713 KB    |            |   0% \\u001b[0m\\u001b[91m\\rreadline-6.2         | 713 KB    | ########9  |  90% \\u001b[0m\\u001b[91m\\rreadline-6.2         | 713 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\ropenssl-1.0.2r       | 3.1 MB    |            |   0% \\u001b[0m\\u001b[91m\\ropenssl-1.0.2r       | 3.1 MB    | #######7   |  78% \\u001b[0m\\u001b[91m\\ropenssl-1.0.2r       | 3.1 MB    | #########8 |  98% \\u001b[0m\\u001b[91m\\ropenssl-1.0.2r       | 3.1 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rca-certificates-2019 | 144 KB    |            |   0% \\u001b[0m\\u001b[91m\\rca-certificates-2019 | 144 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rncurses-5.9          | 1.1 MB    |            |   0% \\u001b[0m\\u001b[91m\\rncurses-5.9          | 1.1 MB    | #######8   |  79% \\u001b[0m\\u001b[91m\\rncurses-5.9          | 1.1 MB    | ########7  |  87% \\u001b[0m\\u001b[91m\\rncurses-5.9          | 1.1 MB    | #########7 |  97% \\u001b[0m\\u001b[91m\\rncurses-5.9          | 1.1 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rlibgcc-ng-9.1.0      | 8.1 MB    |            |   0% \\u001b[0m\\u001b[91m\\rlibgcc-ng-9.1.0      | 8.1 MB    | #######5   |  75% \\u001b[0m\\u001b[91m\\rlibgcc-ng-9.1.0      | 8.1 MB    | #########7 |  98% \\u001b[0m\\u001b[91m\\rlibgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\r\\n\\rsetuptools-41.6.0    | 629 KB    |            |   0% \\u001b[0m\\u001b[91m\\rsetuptools-41.6.0    | 629 KB    | ########7  |  88% \\u001b[0m\\u001b[91m\\rsetuptools-41.6.0    | 629 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rzlib-1.2.11          | 105 KB    |            |   0% \\u001b[0m\\u001b[91m\\rzlib-1.2.11          | 105 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\r_libgcc_mutex-0.1    | 3 KB      |            |   0% \\u001b[0m\\u001b[91m\\r_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rsqlite-3.13.0        | 4.9 MB    |            |   0% \\u001b[0m\\u001b[91m\\rsqlite-3.13.0        | 4.9 MB    | #######5   |  75% \\u001b[0m\\u001b[91m\\rsqlite-3.13.0        | 4.9 MB    | #########7 |  98% \\u001b[0m\\u001b[91m\\rsqlite-3.13.0        | 4.9 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rcertifi-2019.9.11    | 147 KB    |            |   0% \\u001b[0m\\u001b[91m\\rcertifi-2019.9.11    | 147 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rwheel-0.33.6         | 35 KB     |            |   0% \\u001b[0m\\u001b[91m\\rwheel-0.33.6         | 35 KB     | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rtk-8.5.19            | 1.9 MB    |            |   0% \\u001b[0m\\u001b[91m\\rtk-8.5.19            | 1.9 MB    | #######7   |  78% \\u001b[0m\\u001b[91m\\rtk-8.5.19            | 1.9 MB    | #########1 |  91% \\u001b[0m\\u001b[91m\\rtk-8.5.19            | 1.9 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rpip-19.3.1           | 1.9 MB    |            |   0% \\u001b[0m\\u001b[91m\\rpip-19.3.1           | 1.9 MB    | #######7   |  77% \\u001b[0m\\u001b[91m\\rpip-19.3.1           | 1.9 MB    | #########  |  91% \\u001b[0m\\u001b[91m\\rpip-19.3.1           | 1.9 MB    | ########## | 100% \\u001b[0m\\u001b[91m\\r\\n\\rxz-5.2.4             | 366 KB    |            |   0% \\u001b[0m\\u001b[91m\\rxz-5.2.4             | 366 KB    | #########4 |  95% \\u001b[0m\\u001b[91m\\rxz-5.2.4             | 366 KB    | ########## | 100% \\u001b[0m\\u001b[91m\\n\\rpython-3.6.2         | 19.0 MB   |            |   0% \\u001b[0m\\u001b[91m\\rpython-3.6.2         | 19.0 MB   | ##9        |  29% \\u001b[0m\\u001b[91m\\rpython-3.6.2         | 19.0 MB   | ######9    |  70% \\u001b[0m\\u001b[91m\\rpython-3.6.2         | 19.0 MB   | ########8  |  88% \\u001b[0m\\u001b[91m\\rpython-3.6.2         | 19.0 MB   | ########## | 100% \\u001b[0m\\nDownloading and Extracting Packages\\r\\nPreparing transaction: ...working... done\\nVerifying transaction: ...working... done\\r\\nExecuting transaction: ...working... done\\nCollecting transformers==2.0.0\\n  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\\nCollecting azureml-dataprep[fuse,pandas]==1.1.29\\n  Downloading https://files.pythonhosted.org/packages/d5/2d/8e283d6db5156c5ed28fb02d074258409007a77ea52ecb430461470d4f56/azureml_dataprep-1.1.29-py3-none-any.whl (26.9MB)\\nCollecting azureml-defaults\\r\\n  Downloading https://files.pythonhosted.org/packages/f9/ca/14357f9496cde331cf0926505c9ea2e280b887f33e5e0a65322ddda606a9/azureml_defaults-1.0.72-py2.py3-none-any.whl\\nCollecting tensorflow-gpu==2.0.0\\n  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\\nCollecting horovod==0.18.1\\r\\n  Downloading https://files.pythonhosted.org/packages/8b/a0/27b00807e6ed78bcab146594acd680e6493d9e49b43ed1649ccf70e2a95d/horovod-0.18.1.tar.gz (2.8MB)\\nCollecting regex\\r\\n  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\\nCollecting tqdm\\n  Downloading https://files.pythonhosted.org/packages/05/f2/764a5d530cf143ded9bc95216edb6e258c6554511e78de7c250557e8f3ed/tqdm-4.37.0-py2.py3-none-any.whl (53kB)\\nCollecting requests\\n  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\\nCollecting boto3\\n  Downloading https://files.pythonhosted.org/packages/a3/a4/75004e7352a9f3565331e6881a2312a79e46e434c24e5146b0e6c97fa08f/boto3-1.10.9-py2.py3-none-any.whl (128kB)\\nCollecting numpy\\n  Downloading https://files.pythonhosted.org/packages/0e/46/ae6773894f7eacf53308086287897ec568eac9768918d913d5b9d366c5db/numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\\nCollecting sacremoses\\r\\n  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\\nCollecting sentencepiece\\n  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\\nCollecting azureml-dataprep-native<14.0.0,>=13.1.0\\n  Downloading https://files.pythonhosted.org/packages/2a/b8/36399cebff8045a05cb69181771e450101ea3ac14f02ad564d63634b8aca/azureml_dataprep_native-13.1.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\\nCollecting cloudpickle>=1.1.0\\n  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\\nCollecting dotnetcore2>=2.1.9\\n  Downloading https://files.pythonhosted.org/packages/e7/11/699ec6c3ec5e73437834203e4b530fdc760517498208eaa0b37a7f1e10af/dotnetcore2-2.1.9-py3-none-manylinux1_x86_64.whl (29.3MB)\\nCollecting fusepy>=3.0.1; extra == \\\"fuse\\\"\\r\\n  Downloading https://files.pythonhosted.org/packages/04/0b/4506cb2e831cea4b0214d3625430e921faaa05a7fb520458c75a2dbd2152/fusepy-3.0.1.tar.gz\\nCollecting pandas>=0.19.2; extra == \\\"pandas\\\"\\n  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\\nCollecting pyarrow==0.11.*; extra == \\\"pandas\\\"\\n  Downloading https://files.pythonhosted.org/packages/36/94/23135312f97b20d6457294606fb70fad43ef93b7bffe567088ebe3623703/pyarrow-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\\nCollecting azureml-model-management-sdk==1.0.1b6.post1\\r\\n  Downloading https://files.pythonhosted.org/packages/4e/53/9004a1e7d6d4e796abc4bcc8286bfc2a32739c5fbac3859ca7429a228897/azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130kB)\\nCollecting gunicorn==19.9.0\\n  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\\nCollecting applicationinsights>=0.11.7\\n  Downloading https://files.pythonhosted.org/packages/a1/53/234c53004f71f0717d8acd37876e0b65c121181167057b9ce1b1795f96a0/applicationinsights-0.11.9-py2.py3-none-any.whl (58kB)\\nCollecting configparser==3.7.4\\n  Downloading https://files.pythonhosted.org/packages/ba/05/6c96328e92e625fc31445d24d75a2c92ef9ba34fc5b037fe69693c362a0d/configparser-3.7.4-py2.py3-none-any.whl\\nCollecting azureml-core==1.0.72.*\\n  Downloading https://files.pythonhosted.org/packages/b3/40/5f3478937e925a44a92c343b97f71243e9bcbedebc50f67038df044c97d9/azureml_core-1.0.72-py2.py3-none-any.whl (1.1MB)\\nCollecting flask==1.0.3\\n  Downloading https://files.pythonhosted.org/packages/9a/74/670ae9737d14114753b8c8fdf2e8bd212a05d3b361ab15b44937dfd40985/Flask-1.0.3-py2.py3-none-any.whl (92kB)\\nCollecting json-logging-py==0.2\\n  Downloading https://files.pythonhosted.org/packages/e9/e1/46c70eebf216b830867c4896ee678cb7f1b28bb68a2810c7e9a811cecfbc/json-logging-py-0.2.tar.gz\\nCollecting absl-py>=0.7.0\\n  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\\nRequirement already satisfied: wheel>=0.26 in /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0->-r /azureml-environment-setup/condaenv.mh4fscwv.requirements.txt (line 4)) (0.33.6)\\nCollecting opt-einsum>=2.3.2\\n  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\\nCollecting keras-preprocessing>=1.0.5\\n  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\\nCollecting tensorboard<2.1.0,>=2.0.0\\n  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\\nCollecting wrapt>=1.11.1\\r\\n  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\\nCollecting astor>=0.6.0\\n  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\\nCollecting protobuf>=3.6.1\\n  Downloading https://files.pythonhosted.org/packages/a8/52/d8d2dbff74b8bf517c42db8d44c3f9ef6555e6f5d6caddfa3f207b9143df/protobuf-3.10.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\\nCollecting keras-applications>=1.0.8\\n  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\\nCollecting six>=1.10.0\\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\\nCollecting grpcio>=1.8.6\\n  Downloading https://files.pythonhosted.org/packages/30/54/c9810421e41ec0bca2228c6f06b1b1189b196b69533cbcac9f71b44727f8/grpcio-1.24.3-cp36-cp36m-manylinux2010_x86_64.whl (2.2MB)\\nCollecting termcolor>=1.1.0\\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\\nCollecting gast==0.2.2\\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\\nCollecting google-pasta>=0.1.6\\r\\n  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\\nCollecting tensorflow-estimator<2.1.0,>=2.0.0\\n  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\\nCollecting psutil\\n  Downloading https://files.pythonhosted.org/packages/47/ea/d3b6d6fd0b4a6c12984df652525f394e68c8678d2b05075219144eb3a1cf/psutil-5.6.4.tar.gz (447kB)\\n  Installing build dependencies: started\\n  Installing build dependencies: finished with status 'done'\\r\\n  Getting requirements to build wheel: started\\n  Getting requirements to build wheel: finished with status 'done'\\n    Preparing wheel metadata: started\\n    Preparing wheel metadata: finished with status 'done'\\nCollecting pyyaml\\n  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\\nCollecting cffi>=1.4.0\\n  Downloading https://files.pythonhosted.org/packages/49/72/0d42f94fe94afa8030350c26e9d787219f3f008ec9bf6b86c66532b29236/cffi-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (397kB)\\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\\n  Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\\nRequirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib/python3.6/site-packages (from requests->transformers==2.0.0->-r /azureml-environment-setup/condaenv.mh4fscwv.requirements.txt (line 1)) (2019.9.11)\\nCollecting idna<2.9,>=2.5\\n  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\\nCollecting chardet<3.1.0,>=3.0.2\\n  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\\nCollecting s3transfer<0.3.0,>=0.2.0\\n  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\\nCollecting botocore<1.14.0,>=1.13.9\\r\\n  Downloading https://files.pythonhosted.org/packages/bd/45/042a6db7d025be8b04bd47528bc8db414de02e2e5802142e9fb45610d620/botocore-1.13.9-py2.py3-none-any.whl (5.3MB)\\nCollecting jmespath<1.0.0,>=0.7.1\\n  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\\nCollecting click\\n  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\\nCollecting joblib\\n  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\\nCollecting distro>=1.2.0\\n  Downloading https://files.pythonhosted.org/packages/ea/35/82f79b92fa4d937146c660a6482cee4f3dfa1f97ff3d2a6f3ecba33e712e/distro-1.4.0-py2.py3-none-any.whl\\nCollecting python-dateutil>=2.6.1\\n  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\\nCollecting pytz>=2017.2\\n  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\\nCollecting adal>=0.4.5\\n  Downloading https://files.pythonhosted.org/packages/4f/b5/3ea9ae3d1096b9ff31e8f1846c47d49f3129a12464ac0a73b602de458298/adal-1.2.2-py2.py3-none-any.whl (53kB)\\nCollecting liac-arff>=2.1.1\\n  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\\nCollecting dill>=0.2.7.1\\n  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\\nCollecting azure-graphrbac>=0.40.0\\r\\n  Downloading https://files.pythonhosted.org/packages/3e/93/02056aca45162f9fc275d1eaad12a2a07ef92375afb48eabddc4134b8315/azure_graphrbac-0.61.1-py2.py3-none-any.whl (141kB)\\nCollecting msrest>=0.5.1\\n  Downloading https://files.pythonhosted.org/packages/27/b0/c34b3ea9b2ed74b800520fbefb312cdb7f05c20b8bd42e5e7662a5614f98/msrest-0.6.10-py2.py3-none-any.whl (82kB)\\nCollecting contextlib2\\n  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\\nCollecting docker\\n  Downloading https://files.pythonhosted.org/packages/cc/ca/699d4754a932787ef353a157ada74efd1ceb6d1fc0bfb7989ae1e7b33111/docker-4.1.0-py2.py3-none-any.whl (139kB)\\nCollecting azure-mgmt-storage>=1.5.0\\n  Downloading https://files.pythonhosted.org/packages/8a/d9/117216e5f671f6c3238c50cba583924252c5ee08091a7d10fa1d3113faa3/azure_mgmt_storage-6.0.0-py2.py3-none-any.whl (514kB)\\nCollecting pyopenssl\\n  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\\nCollecting backports.tempfile\\n  Downloading https://files.pythonhosted.org/packages/b4/5c/077f910632476281428fe254807952eb47ca78e720d059a46178c541e669/backports.tempfile-1.0-py2.py3-none-any.whl\\nCollecting ndg-httpsclient\\n  Downloading https://files.pythonhosted.org/packages/fb/67/c2f508c00ed2a6911541494504b7cac16fe0b0473912568df65fd1801132/ndg_httpsclient-0.5.1-py3-none-any.whl\\nCollecting azure-mgmt-authorization>=0.40.0\\n  Downloading https://files.pythonhosted.org/packages/5e/17/4724694ddb3311955ddc367eddcd0928f8ee2c7b12d5a6f0b12bca0b03db/azure_mgmt_authorization-0.60.0-py2.py3-none-any.whl (82kB)\\nCollecting SecretStorage\\n  Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\\nCollecting jsonpickle\\n  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\\nCollecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\\n  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\\nCollecting PyJWT\\n  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\\nCollecting ruamel.yaml<=0.15.89,>=0.15.35\\r\\n  Downloading https://files.pythonhosted.org/packages/36/e1/cc2fa400fa5ffde3efa834ceb15c464075586de05ca3c553753dcd6f1d3b/ruamel.yaml-0.15.89-cp36-cp36m-manylinux1_x86_64.whl (651kB)\\nCollecting pathspec\\n  Downloading https://files.pythonhosted.org/packages/7a/68/5902e8cd7f7b17c5879982a3a3ee2ad0c3b92b80c79989a2d3e1ca8d29e1/pathspec-0.6.0.tar.gz\\nCollecting msrestazure>=0.4.33\\n  Downloading https://files.pythonhosted.org/packages/68/75/5cb56ca8cbc6c5fe476e4878c73f57a331edcf55e5d3fcb4a7377d7d659d/msrestazure-0.6.2-py2.py3-none-any.whl (40kB)\\nCollecting azure-mgmt-resource>=1.2.1\\n  Downloading https://files.pythonhosted.org/packages/7c/0d/80815326fa04f2a73ea94b0f57c29669c89df5aa5f5e285952f6445a91c4/azure_mgmt_resource-5.1.0-py2.py3-none-any.whl (681kB)\\nCollecting azure-mgmt-keyvault>=0.40.0\\n  Downloading https://files.pythonhosted.org/packages/b3/d1/9fed0a3a3b43d0b1ad59599b5c836ccc4cf117e26458075385bafe79575b/azure_mgmt_keyvault-2.0.0-py2.py3-none-any.whl (80kB)\\nCollecting azure-mgmt-containerregistry>=2.0.0\\n  Downloading https://files.pythonhosted.org/packages/97/70/8c2d0509db466678eba16fa2b0a539499f3b351b1f2993126ad843d5be13/azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718kB)\\nCollecting azure-common>=1.1.12\\n  Downloading https://files.pythonhosted.org/packages/00/55/a703923c12cd3172d5c007beda0c1a34342a17a6a72779f8a7c269af0cd6/azure_common-1.1.23-py2.py3-none-any.whl\\nCollecting Werkzeug>=0.14\\n  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\\nCollecting itsdangerous>=0.24\\n  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\\nCollecting Jinja2>=2.10\\n  Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\\nCollecting google-auth-oauthlib<0.5,>=0.4.1\\n  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\\nCollecting markdown>=2.6.8\\n  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\\nRequirement already satisfied: setuptools>=41.0.0 in /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->-r /azureml-environment-setup/condaenv.mh4fscwv.requirements.txt (line 4)) (41.6.0.post20191101)\\nCollecting google-auth<2,>=1.6.3\\n  Downloading https://files.pythonhosted.org/packages/2f/81/d1e7d9974ba7c886f6d133a8baae18cb8d92b2d09bcc4f46328306825de0/google_auth-1.7.0-py2.py3-none-any.whl (74kB)\\nCollecting h5py\\n  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\\nCollecting pycparser\\n  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\\nCollecting docutils<0.16,>=0.10\\r\\n  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\\nCollecting isodate>=0.6.0\\n  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\\nCollecting requests-oauthlib>=0.5.0\\n  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\\nCollecting websocket-client>=0.32.0\\n  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\\nCollecting backports.weakref\\n  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\\nCollecting pyasn1>=0.1.1\\n  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\\nCollecting jeepney\\n  Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\\nCollecting MarkupSafe>=0.23\\n  Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\\nCollecting cachetools<3.2,>=2.0.0\\n  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\\nCollecting rsa<4.1,>=3.1.4\\n  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\\nCollecting pyasn1-modules>=0.2.1\\n  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\\nCollecting oauthlib>=3.0.0\\n  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\\nBuilding wheels for collected packages: psutil\\n  Building wheel for psutil (PEP 517): started\\n  Building wheel for psutil (PEP 517): finished with status 'done'\\r\\n  Created wheel for psutil: filename=psutil-5.6.4-cp36-cp36m-linux_x86_64.whl size=274379 sha256=01aaeb57d750872bc0187e9f73de611d602de2071b19571418b7d2db1eec148c\\n  Stored in directory: /root/.cache/pip/wheels/1a/98/c1/36abacc61566e32c22317111d85ca6977c9788ba0cadf8687a\\nSuccessfully built psutil\\nBuilding wheels for collected packages: horovod, sacremoses, fusepy, json-logging-py, absl-py, opt-einsum, wrapt, termcolor, gast, pyyaml, liac-arff, dill, pathspec, pycparser\\n  Building wheel for horovod (setup.py): started\\n  Building wheel for horovod (setup.py): finished with status 'error'\\r\\n  Running setup.py clean for horovod\\n\\u001b[91m  ERROR: Command errored out with exit status 1:\\n   command: /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\\\"'\\\"'/tmp/pip-install-s42uo1fu/horovod/setup.py'\\\"'\\\"'; __file__='\\\"'\\\"'/tmp/pip-install-s42uo1fu/horovod/setup.py'\\\"'\\\"';f=getattr(tokenize, '\\\"'\\\"'open'\\\"'\\\"', open)(__file__);code=f.read().replace('\\\"'\\\"'\\\\r\\\\n'\\\"'\\\"', '\\\"'\\\"'\\\\n'\\\"'\\\"');f.close();exec(compile(code, __file__, '\\\"'\\\"'exec'\\\"'\\\"'))' bdist_wheel -d /tmp/pip-wheel-nfy_y1sl --python-tag cp36\\n       cwd: /tmp/pip-install-s42uo1fu/horovod/\\n  Complete output (220 lines):\\n  In file included from ext/_yaml.c:596:0:\\n  ext/_yaml.h:2:10: fatal error: yaml.h: No such file or directory\\n   #include <yaml.h>\\n            ^~~~~~~~\\n  compilation terminated.\\n  Error compiling module, falling back to pure Python\\n  zip_safe flag not set; analyzing archive contents...\\n  \\n  Installed /tmp/pip-install-s42uo1fu/horovod/.eggs/PyYAML-5.1.2-py3.6-linux-x86_64.egg\\n  Searching for psutil\\n  Reading https://pypi.org/simple/psutil/\\n  Downloading https://files.pythonhosted.org/packages/47/ea/d3b6d6fd0b4a6c12984df652525f394e68c8678d2b05075219144eb3a1cf/psutil-5.6.4.tar.gz#sha256=512e854d68f8b42f79b2c7864d997b39125baff9bcff00028ce43543867de7c4\\n  Best match: psutil 5.6.4\\n  Processing psutil-5.6.4.tar.gz\\n  Writing /tmp/easy_install-sl1xieai/psutil-5.6.4/setup.cfg\\n  Running psutil-5.6.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-sl1xieai/psutil-5.6.4/egg-dist-tmp-be1ztjcy\\n  creating /tmp/pip-install-s42uo1fu/horovod/.eggs/psutil-5.6.4-py3.6-linux-x86_64.egg\\n  Extracting psutil-5.6.4-py3.6-linux-x86_64.egg to /tmp/pip-install-s42uo1fu/horovod/.eggs\\n  \\n  Installed /tmp/pip-install-s42uo1fu/horovod/.eggs/psutil-5.6.4-py3.6-linux-x86_64.egg\\n  Searching for cloudpickle\\n  Reading https://pypi.org/simple/cloudpickle/\\n  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl#sha256=f3ef2c9d438f1553ce7795afb18c1f190d8146132496169ef6aa9b7b65caa4c3\\n  Best match: cloudpickle 1.2.2\\n  Processing cloudpickle-1.2.2-py2.py3-none-any.whl\\n  Installing cloudpickle-1.2.2-py2.py3-none-any.whl to /tmp/pip-install-s42uo1fu/horovod/.eggs\\n  \\n  Installed /tmp/pip-install-s42uo1fu/horovod/.eggs/cloudpickle-1.2.2-py3.6.egg\\n  Searching for pycparser\\n  Reading https://pypi.org/simple/pycparser/\\n  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz#sha256=a988718abfad80b6b157acce7bf130a30876d27603738ac39f140993246b25b3\\n  Best match: pycparser 2.19\\n  Processing pycparser-2.19.tar.gz\\n  Writing /tmp/easy_install-de7rwtbf/pycparser-2.19/setup.cfg\\n  Running pycparser-2.19/setup.py -q bdist_egg --dist-dir /tmp/easy_install-de7rwtbf/pycparser-2.19/egg-dist-tmp-bgl_7iqb\\n  warning: no previously-included files found matching 'setup.pyc'\\n  warning: no previously-included files matching 'yacctab.*' found under directory 'tests'\\n  warning: no previously-included files matching 'lextab.*' found under directory 'tests'\\n  warning: no previously-included files matching 'yacctab.*' found under directory 'examples'\\n  warning: no previously-included files matching 'lextab.*' found under directory 'examples'\\n  zip_safe flag not set; analyzing archive contents...\\n  pycparser.ply.__pycache__.lex.cpython-36: module references __file__\\n  pycparser.ply.__pycache__.lex.cpython-36: module MAY be using inspect.getsourcefile\\n  pycparser.ply.__pycache__.yacc.cpython-36: module references __file__\\n  pycparser.ply.__pycache__.yacc.cpython-36: module MAY be using inspect.getsourcefile\\n  pycparser.ply.__pycache__.yacc.cpython-36: module MAY be using inspect.stack\\n  pycparser.ply.__pycache__.ygen.cpython-36: module references __file__\\n  creating /tmp/pip-install-s42uo1fu/horovod/.eggs/pycparser-2.19-py3.6.egg\\n  Extracting pycparser-2.19-py3.6.egg to /tmp/pip-install-s42uo1fu/horovod/.eggs\\n  \\n  Installed /tmp/pip-install-s42uo1fu/horovod/.eggs/pycparser-2.19-py3.6.egg\\n  running bdist_wheel\\n  running build\\n  running build_py\\n  creating build\\n  creating build/lib.linux-x86_64-3.6\\n  creating build/lib.linux-x86_64-3.6/horovod\\n  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod\\n  creating build/lib.linux-x86_64-3.6/horovod/torch\\n  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch\\n  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch\\n  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch\\n  creating build/lib.linux-x86_64-3.6/horovod/spark\\n  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark\\n  creating build/lib.linux-x86_64-3.6/horovod/_keras\\n  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras\\n  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras\\n  creating build/lib.linux-x86_64-3.6/horovod/common\\n  copying horovod/common/basics.py -> build/lib.linux-x86_64-3.6/horovod/common\\n  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common\\n  copying horovod/common/util.py -> build/lib.linux-x86_64-3.6/horovod/common\\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow\\n  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\\n  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\\n  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\\n  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\\n  creating build/lib.linux-x86_64-3.6/horovod/mxnet\\n  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\\n  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\\n  creating build/lib.linux-x86_64-3.6/horovod/run\\n  copying horovod/run/run.py -> build/lib.linux-x86_64-3.6/horovod/run\\n  copying horovod/run/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/run\\n  copying horovod/run/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run\\n  copying horovod/run/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/run\\n  copying horovod/run/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/run\\n  creating build/lib.linux-x86_64-3.6/horovod/keras\\n  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras\\n  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras\\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\\n  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\\n  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\\n  creating build/lib.linux-x86_64-3.6/horovod/spark/driver\\n  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\\n  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\\n  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\\n  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\\n  creating build/lib.linux-x86_64-3.6/horovod/spark/task\\n  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\\n  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\\n  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\\n  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\\n  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\\n  creating build/lib.linux-x86_64-3.6/horovod/run/driver\\n  copying horovod/run/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\\n  copying horovod/run/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\\n  creating build/lib.linux-x86_64-3.6/horovod/run/common\\n  copying horovod/run/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common\\n  creating build/lib.linux-x86_64-3.6/horovod/run/task\\n  copying horovod/run/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/task\\n  copying horovod/run/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/task\\n  creating build/lib.linux-x86_64-3.6/horovod/run/util\\n  copying horovod/run/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/run/util\\n  copying horovod/run/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/util\\n  copying horovod/run/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/run/util\\n  copying horovod/run/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/util\\n  creating build/lib.linux-x86_64-3.6/horovod/run/rendezvous\\n  copying horovod/run/rendezvous/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/rendezvous\\n  copying horovod/run/rendezvous/http_server.py -> build/lib.linux-x86_64-3.6/horovod/run/rendezvous\\n  creating build/lib.linux-x86_64-3.6/horovod/run/common/service\\n  copying horovod/run/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\\n  copying horovod/run/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\\n  copying horovod/run/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\\n  creating build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/settings.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/env.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/config_parser.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  copying horovod/run/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\\n  running build_ext\\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o\\n  cc1plus: warning: command line option \\u2018-Wstrict-prototypes\\u2019 is valid for C/ObjC but not for C++\\n  gcc -pthread -shared -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.so\\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_link_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o\\n  cc1plus: warning: command line option \\u2018-Wstrict-prototypes\\u2019 is valid for C/ObjC but not for C++\\n  gcc -pthread -shared -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed -Wl,--version-script=horovod.lds build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.so\\n  INFO: Cannot find CMake, will skip compiling Horovod with Gloo.\\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/usr/local/cuda/include -I/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cuda.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cuda.o\\n  cc1plus: warning: command line option \\u2018-Wstrict-prototypes\\u2019 is valid for C/ObjC but not for C++\\n  gcc -pthread -shared -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_cuda.o -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -lcudart -o build/temp.linux-x86_64-3.6/test_compile/test_cuda.so\\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/usr/local/cuda/include -I/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_nccl.cc -o build/temp.linux-x86_64-3.6/test_compile/test_nccl.o\\n  cc1plus: warning: command line option \\u2018-Wstrict-prototypes\\u2019 is valid for C/ObjC but not for C++\\n  gcc -pthread -shared -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -Wl,-rpath=/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_nccl.o -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -L/azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib -lnccl_static -o build/temp.linux-x86_64-3.6/test_compile/test_nccl.so\\n  INFO: Unable to build TensorFlow plugin, will skip it.\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 72, in check_tf_version\\n      import tensorflow as tf\\n  ModuleNotFoundError: No module named 'tensorflow'\\n  \\n  During handling of the above exception, another exception occurred:\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1375, in build_extensions\\n      build_tf_extension(self, options)\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 851, in build_tf_extension\\n      check_tf_version()\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 79, in check_tf_version\\n      'import tensorflow failed, is it installed?\\\\n\\\\n%s' % traceback.format_exc())\\n  distutils.errors.DistutilsPlatformError: import tensorflow failed, is it installed?\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 72, in check_tf_version\\n      import tensorflow as tf\\n  ModuleNotFoundError: No module named 'tensorflow'\\n  \\n  \\n  INFO: Unable to build PyTorch plugin, will skip it.\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1060, in check_torch_version\\n      import torch\\n  ModuleNotFoundError: No module named 'torch'\\n  \\n  During handling of the above exception, another exception occurred:\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1387, in build_extensions\\n      torch_version = check_torch_version()\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1067, in check_torch_version\\n      'import torch failed, is it installed?\\\\n\\\\n%s' % traceback.format_exc())\\n  distutils.errors.DistutilsPlatformError: import torch failed, is it installed?\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1060, in check_torch_version\\n      import torch\\n  ModuleNotFoundError: No module named 'torch'\\n  \\n  \\n  INFO: Unable to build MXNet plugin, will skip it.\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 88, in check_mx_version\\n      import mxnet as mx\\n  ModuleNotFoundError: No module named 'mxnet'\\n  \\n  During handling of the above exception, another exception occurred:\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1403, in build_extensions\\n      build_mx_extension(self, options)\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 1000, in build_mx_extension\\n      check_mx_version()\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 95, in check_mx_version\\n      'import mxnet failed, is it installed?\\\\n\\\\n%s' % traceback.format_exc())\\n  distutils.errors.DistutilsPlatformError: import mxnet failed, is it installed?\\n  \\n  Traceback (most recent call last):\\n    File \\\"/tmp/pip-install-s42uo1fu/horovod/setup.py\\\", line 88, in check_mx_version\\n      import mxnet as mx\\n  ModuleNotFoundError: No module named 'mxnet'\\n  \\n  \\n  error: None of TensorFlow, PyTorch, or MXNet plugins were built. See errors above.\\n  ----------------------------------------\\n  ERROR: Failed building wheel for horovod\\r\\n\\u001b[0m  Building wheel for sacremoses (setup.py): started\\n  Building wheel for sacremoses (setup.py): finished with status 'done'\\n  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=22d08ad0d9ffaf465f1f3f9520edf52f6f383e7cc707e14567f209d1bb10d74a\\n  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\\n  Building wheel for fusepy (setup.py): started\\n  Building wheel for fusepy (setup.py): finished with status 'done'\\n  Created wheel for fusepy: filename=fusepy-3.0.1-cp36-none-any.whl size=10505 sha256=1c8979ab1980a7adfbb2a0489c0ee673f789be22acdd0b60cf58bf320a8b7119\\n  Stored in directory: /root/.cache/pip/wheels/4c/a5/91/7772af9e21c461f07bb40f26d928d7d231d224977dd8353bab\\n  Building wheel for json-logging-py (setup.py): started\\n  Building wheel for json-logging-py (setup.py): finished with status 'done'\\n  Created wheel for json-logging-py: filename=json_logging_py-0.2-cp36-none-any.whl size=3924 sha256=760a9fe1f5f87956b3fffe6f66563350340635c57c462319f3e85a1997b71d58\\n  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\\n  Building wheel for absl-py (setup.py): started\\n  Building wheel for absl-py (setup.py): finished with status 'done'\\n  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=13c57fcf8f4d15ad5457def6dcdd15a24bbb6739ef6dd56002df4922267deca6\\n  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\\n  Building wheel for opt-einsum (setup.py): started\\r\\n  Building wheel for opt-einsum (setup.py): finished with status 'done'\\n  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=61682 sha256=57ab11d771663c98ba40987d761619757f8ab1d904b7344074a71bea2ab189f6\\n  Stored in directory: /root/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\\n  Building wheel for wrapt (setup.py): started\\n  Building wheel for wrapt (setup.py): finished with status 'done'\\n  Created wheel for wrapt: filename=wrapt-1.11.2-cp36-cp36m-linux_x86_64.whl size=69926 sha256=304ac5c0cff98ac7fd60e3a02ce6b20c70843c824bb752ed61aeca997d507aed\\n  Stored in directory: /root/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\\n  Building wheel for termcolor (setup.py): started\\n  Building wheel for termcolor (setup.py): finished with status 'done'\\n  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=a2e24ff8aec62a7dd7aafac4bde3afbb524bbc786e29814c6f0206bb3f91631f\\n  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\\n  Building wheel for gast (setup.py): started\\n  Building wheel for gast (setup.py): finished with status 'done'\\r\\n  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=0c48bed06277ddd75bbc25964b6215c013f723d2026145c4eb56fe4b562838b8\\n  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\\n  Building wheel for pyyaml (setup.py): started\\n  Building wheel for pyyaml (setup.py): finished with status 'done'\\n  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=0fbc1f1c255f8ae62e1aa67d6e8204998957b549edf40a96a9c8c8c971c05c93\\n  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\\n  Building wheel for liac-arff (setup.py): started\\n  Building wheel for liac-arff (setup.py): finished with status 'done'\\n  Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13333 sha256=a7a20a3d71ad3b31bdb44bc5cd2193735ee84615cd039fd2170e2df6d321bfc8\\n  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\\n  Building wheel for dill (setup.py): started\\n  Building wheel for dill (setup.py): finished with status 'done'\\n  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=8a08aca0b073fffb633d233ccd2fe94ed8887903e03907efff76ee85f01af4b1\\n  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\\n  Building wheel for pathspec (setup.py): started\\n  Building wheel for pathspec (setup.py): finished with status 'done'\\n  Created wheel for pathspec: filename=pathspec-0.6.0-cp36-none-any.whl size=26671 sha256=2b19abc41057836d66a030b25a526ccf954d3c8450be7eafa1388ec7dd3e21a5\\n  Stored in directory: /root/.cache/pip/wheels/62/b8/e1/e2719465b5947c40cd85d613d6cb33449b86a1ca5a6c574269\\n  Building wheel for pycparser (setup.py): started\\n  Building wheel for pycparser (setup.py): finished with status 'done'\\r\\n  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111029 sha256=2ea8046a43975c4bccb04a855d6d0b30ee94d9820624b5d9c1f70d8798aa9290\\n  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\\nSuccessfully built sacremoses fusepy json-logging-py absl-py opt-einsum wrapt termcolor gast pyyaml liac-arff dill pathspec pycparser\\nFailed to build horovod\\n\\u001b[91mERROR: botocore 1.13.9 has requirement python-dateutil<2.8.1,>=2.1; python_version >= \\\"2.7\\\", but you'll have python-dateutil 2.8.1 which is incompatible.\\n\\u001b[0mInstalling collected packages: regex, tqdm, urllib3, idna, chardet, requests, six, python-dateutil, jmespath, docutils, botocore, s3transfer, boto3, numpy, click, joblib, sacremoses, sentencepiece, transformers, azureml-dataprep-native, cloudpickle, distro, dotnetcore2, fusepy, pytz, pandas, pyarrow, azureml-dataprep, PyJWT, pycparser, cffi, cryptography, adal, liac-arff, dill, azureml-model-management-sdk, gunicorn, applicationinsights, configparser, isodate, oauthlib, requests-oauthlib, msrest, azure-common, msrestazure, azure-graphrbac, contextlib2, websocket-client, docker, azure-mgmt-storage, pyopenssl, backports.weakref, backports.tempfile, pyasn1, ndg-httpsclient, azure-mgmt-authorization, jeepney, SecretStorage, jsonpickle, ruamel.yaml, pathspec, azure-mgmt-resource, azure-mgmt-keyvault, azure-mgmt-containerregistry, azureml-core, Werkzeug, itsdangerous, MarkupSafe, Jinja2, flask, json-logging-py, azureml-defaults, absl-py, opt-einsum, keras-preprocessing, cachetools, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, markdown, grpcio, protobuf, tensorboard, wrapt, astor, h5py, keras-applications, termcolor, gast, google-pasta, tensorflow-estimator, tensorflow-gpu, psutil, pyyaml, horovod\\n    Running setup.py install for horovod: started\\r\\n    Running setup.py install for horovod: still running...\\r\\n    Running setup.py install for horovod: finished with status 'done'\\r\\nSuccessfully installed Jinja2-2.10.3 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.1.1 Werkzeug-0.16.0 absl-py-0.8.1 adal-1.2.2 applicationinsights-0.11.9 astor-0.8.0 azure-common-1.1.23 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.0.0 azure-mgmt-resource-5.1.0 azure-mgmt-storage-6.0.0 azureml-core-1.0.72 azureml-dataprep-1.1.29 azureml-dataprep-native-13.1.0 azureml-defaults-1.0.72 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 boto3-1.10.9 botocore-1.13.9 cachetools-3.1.1 cffi-1.13.2 chardet-3.0.4 click-7.0 cloudpickle-1.2.2 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-2.8 dill-0.3.1.1 distro-1.4.0 docker-4.1.0 docutils-0.15.2 dotnetcore2-2.1.9 flask-1.0.3 fusepy-3.0.1 gast-0.2.2 google-auth-1.7.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.24.3 gunicorn-19.9.0 h5py-2.10.0 horovod-0.18.1 idna-2.8 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.4.1 jmespath-0.9.4 joblib-0.14.0 json-logging-py-0.2 jsonpickle-1.2 keras-applications-1.0.8 keras-preprocessing-1.1.0 liac-arff-2.4.0 markdown-3.1.1 msrest-0.6.10 msrestazure-0.6.2 ndg-httpsclient-0.5.1 numpy-1.17.3 oauthlib-3.1.0 opt-einsum-3.1.0 pandas-0.25.3 pathspec-0.6.0 protobuf-3.10.0 psutil-5.6.4 pyarrow-0.11.1 pyasn1-0.4.7 pyasn1-modules-0.2.7 pycparser-2.19 pyopenssl-19.0.0 python-dateutil-2.8.1 pytz-2019.3 pyyaml-5.1.2 regex-2019.11.1 requests-2.22.0 requests-oauthlib-1.2.0 rsa-4.0 ruamel.yaml-0.15.89 s3transfer-0.2.1 sacremoses-0.0.35 sentencepiece-0.1.83 six-1.12.0 tensorboard-2.0.1 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0 termcolor-1.1.0 tqdm-4.37.0 transformers-2.0.0 urllib3-1.25.6 websocket-client-0.56.0 wrapt-1.11.2\\n\\u001b[91m\\n\\u001b[0m#\\n# To activate this environment, use:\\n# > source activate /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817\\n#\\n# To deactivate an active environment, use:\\n# > source deactivate\\n#\\n\\nRemoving intermediate container 16ad378da3e0\\n ---> 8fb665df847e\\nStep 9/14 : ENV PATH /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/bin:$PATH\\r\\n ---> Running in acc6ce8f4345\\nRemoving intermediate container acc6ce8f4345\\n ---> e2cf3facb461\\nStep 10/14 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817\\r\\n ---> Running in a2b6d91e7938\\nRemoving intermediate container a2b6d91e7938\\n ---> be71acf63d80\\nStep 11/14 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_b705a44b160b7c072bc83870a99f2817/lib:$LD_LIBRARY_PATH\\r\\n ---> Running in aeda99bc41c2\\nRemoving intermediate container aeda99bc41c2\\n ---> 8b23d11b9f6c\\nStep 12/14 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\\r\\n ---> 919edcd35172\\nStep 13/14 : ENV AZUREML_ENVIRONMENT_IMAGE True\\r\\n ---> Running in 91c34529fb4c\\nRemoving intermediate container 91c34529fb4c\\n ---> 2e840614158e\\r\\nStep 14/14 : CMD [\\\"bash\\\"]\\n ---> Running in 0eedf38eaaaa\\nRemoving intermediate container 0eedf38eaaaa\\n ---> e5b0de0e4915\\r\\nSuccessfully built e5b0de0e4915\\nSuccessfully tagged tfworld0b69c845.azurecr.io/azureml/azureml_8b8765b28a4a858399aa44e9096a8d04:latest\\n2019/11/05 08:10:37 Successfully executed container: acb_step_0\\n2019/11/05 08:10:37 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2019/11/05 08:10:37 Pushing image: tfworld0b69c845.azurecr.io/azureml/azureml_8b8765b28a4a858399aa44e9096a8d04:latest, attempt 1\\nThe push refers to repository [tfworld0b69c845.azurecr.io/azureml/azureml_8b8765b28a4a858399aa44e9096a8d04]\\n3743e8ca1060: Preparing\\nb6361dd54d01: Preparing\\nd13b5717cd37: Preparing\\nd5afe613e12e: Preparing\\n80be8e3af563: Preparing\\nb46cd0d11e4d: Preparing\\n5d80dfe95b9f: Preparing\\n88dba4d4b1b3: Preparing\\ndd1cdb245341: Preparing\\n632ac93df444: Preparing\\n85355e262b10: Preparing\\n5c11f617277f: Preparing\\n2f3abd252a36: Preparing\\n2eafd5e86d56: Preparing\\n1673fa18caaf: Preparing\\n7545d8b4edec: Preparing\\n718bbdc0b45f: Preparing\\n4a78de7ea906: Preparing\\n0bfa7a55184c: Preparing\\n122be11ab4a2: Preparing\\n7beb13bce073: Preparing\\nf7eae43028b3: Preparing\\n6cebf3abed5f: Preparing\\n2eafd5e86d56: Waiting\\n1673fa18caaf: Waiting\\n7545d8b4edec: Waiting\\n718bbdc0b45f: Waiting\\n4a78de7ea906: Waiting\\n0bfa7a55184c: Waiting\\n122be11ab4a2: Waiting\\n7beb13bce073: Waiting\\nf7eae43028b3: Waiting\\n6cebf3abed5f: Waiting\\ndd1cdb245341: Waiting\\n632ac93df444: Waiting\\n85355e262b10: Waiting\\n5c11f617277f: Waiting\\n2f3abd252a36: Waiting\\nb46cd0d11e4d: Waiting\\n5d80dfe95b9f: Waiting\\n88dba4d4b1b3: Waiting\\n3743e8ca1060: Pushed\\n80be8e3af563: Pushed\\nd5afe613e12e: Pushed\\r\\nd13b5717cd37: Pushed\\nb46cd0d11e4d: Pushed\\n5d80dfe95b9f: Pushed\\n88dba4d4b1b3: Pushed\\r\\ndd1cdb245341: Pushed\\r\\n85355e262b10: Pushed\\r\\n2f3abd252a36: Pushed\\r\\n5c11f617277f: Pushed\\r\\n632ac93df444: Pushed\\r\\n718bbdc0b45f: Pushed\\r\\n4a78de7ea906: Pushed\\r\\n0bfa7a55184c: Pushed\\r\\n122be11ab4a2: Pushed\\r\\n7beb13bce073: Pushed\\r\\nf7eae43028b3: Pushed\\r\\n6cebf3abed5f: Pushed\\r\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cancel a run at anytime which will stop the run and scale down the nodes in the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run1.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we wait for the run to complete, let's go over how a Run is executed in Azure Machine Learning.\n",
    "\n",
    "![](./images/aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Add Metrics Logging\n",
    "\n",
    "So we were able to clone a Tensorflow 2.0 project and run it without any changes. However, with larger scale projects we would want to log some metrics in order to make it easier to monitor the performance of our model. \n",
    "\n",
    "We can do this by adding a few lines of code into our training script:\n",
    "\n",
    "```python\n",
    "# 1) Import SDK Run object\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# 2) Get current service context\n",
    "run = Run.get_context()\n",
    "\n",
    "# 3) Log the metrics that we want\n",
    "run.log('val_accuracy', float(logs.get('val_accuracy')))\n",
    "run.log('accuracy', float(logs.get('accuracy')))\n",
    "```\n",
    "We've created a *train_logging.py* script that includes logging metrics as shown above. \n",
    "\n",
    "*  **ACTION**: Explore _train_logging.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did before. \n",
    "\n",
    "*Since our cluster can scale automatically to two nodes, we can run this job simultaneously with the previous one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator2 = TensorFlow(source_directory='.',\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target, \n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/model'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])\n",
    "\n",
    "run2 = experiment.submit(estimator2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we view the current details of the run, you will notice that the metrics will be logged into graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Monitoring metrics with Tensorboard\n",
    "\n",
    "Tensorboard is a popular Deep Learning Training visualization tool and it's built-in into TensorFlow framework. We can easily add tracking of the metrics in Tensorboard format by adding Tensorboard callback to the **fit** function call.\n",
    "```python\n",
    "    # Add callback to record Tensorboard events\n",
    "    model.fit(train_dataset, epochs=FLAGS.num_epochs, \n",
    "              steps_per_epoch=FLAGS.steps_per_epoch, validation_data=valid_dataset, \n",
    "              callbacks=[\n",
    "                  AmlLogger(),\n",
    "                  tf.keras.callbacks.TensorBoard(update_freq='batch')]\n",
    "             )\n",
    "```\n",
    "\n",
    "#### Launch Tensorboard\n",
    "Azure ML service provides built-in integration with Tensorboard through **tensorboard** package.\n",
    "\n",
    "While the run is in progress (or after it has completed), we can start Tensorboard with the run as its target, and it will begin streaming logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run2])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Tensorboard\n",
    "When you're done, make sure to call the stop() method of the Tensorboard object, or it will stay running even after your job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model performance\n",
    "\n",
    "Last training run produced model of decent accuracy. Let's test it out and see what it does. First, let's check what files our latest training run produced and download the model files.\n",
    "\n",
    "#### Download model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.download_files(prefix='outputs/model')\n",
    "\n",
    "# If you haven't finished training the model then just download pre-made model from datastore\n",
    "datastore.download('./',prefix=\"azure-service-classifier/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model\n",
    "\n",
    "Next step is to import our model class and instantiate fine-tuned model from the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TFBertForMultiClassification\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "def encode_example(text, max_seq_length):\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "        )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "    \n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "# Load model and tokenizer\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained('azure-service-classifier/model', num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define prediction function\n",
    "\n",
    "Using the model object we can interpret new questions and predict what Azure service they talk about. To do that conveniently we'll define **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(question):\n",
    "    input_ids, attention_mask, token_type_ids = encode_example(question, 128)\n",
    "    predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "    prediction = labels[predictions[0].argmax().item()]\n",
    "    probability = predictions[0].max()\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "    print('Prediction: {}'.format(prediction))\n",
    "    print('Probability: {}'.format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiement with our new model\n",
    "\n",
    "Now we can easily test responses of the model to new inputs. \n",
    "*  **ACTION**: Invent yout own input for one of the 5 services our model understands: 'azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route question\n",
    "predict(\"How can I specify Service Principal in devops pipeline when deploying virtual machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now more tricky cae - the opposite\n",
    "predict(\"How can virtual machine trigger devops pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training Across Multiple GPUs\n",
    "\n",
    "Distributed training allows us to train across multiple nodes if your cluster allows it. Azure Machine Learning service helps manage the infrastructure for training distributed jobs. All we have to do is add the following parameters to our estimator object in order to enable this:\n",
    "\n",
    "- `node_count`: The number of nodes to run this job across. Our cluster has a maximum node limit of 2, so we can set this number up to 2.\n",
    "- `process_count_per_node`: The number of processes to enable per node. The nodes in our cluster have 2 GPUs each. We will set this value to 2 which will allow us to distribute the load on both GPUs. Using multi-GPUs nodes is benefitial as communication channel bandwidth on local machine is higher.\n",
    "- `distributed_training`: The backend to use for our distributed job. We will be using an MPI (Message Passing Interface) backend which is used by Horovod framework.\n",
    "\n",
    "We use [Horovod](https://github.com/horovod/horovod), which is a framework that allows us to easily modifying our existing training script to be run across multiple nodes/GPUs. The distributed training script is saved as *train_horovod.py*.\n",
    "\n",
    "*  **ACTION**: Explore _train_horovod.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did with the others, but with the additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import Mpi\n",
    "\n",
    "estimator3 = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_horovod.py',compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/model'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        node_count=1,\n",
    "                        distributed_training=Mpi(process_count_per_node=2),\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])\n",
    "\n",
    "run3 = experiment.submit(estimator3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can view the current details of the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run completes note the time it took. It should be around 5 minutes. As you can see, by moving to the cloud GPUs and using distibuted training we managed to reduce training time of our model from more than an hour to 5 minutes. This greatly improves speed of experimentation and innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters Using Hyperdrive\n",
    "\n",
    "So far we have been putting in default hyperparameter values, but in practice we would need tune these values to optimize the performance. Azure Machine Learning service provides many methods for tuning hyperparameters using different strategies.\n",
    "\n",
    "The first step is to choose the parameter space that we want to search. We have a few choices to make here :\n",
    "\n",
    "- **Parameter Sampling Method**: This is how we select the combinations of parameters to sample. Azure Machine Learning service offers [RandomParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py), [GridParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.gridparametersampling?view=azure-ml-py), and [BayesianParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersampling?view=azure-ml-py). We will use the `GridParameterSampling` method.\n",
    "- **Parameters To Search**: We will be searching for optimal combinations of `learning_rate` and `num_epochs`.\n",
    "- **Parameter Expressions**: This defines the [functions that can be used to describe a hyperparameter search space](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py), which can be discrete or continuous. We will be using a `discrete set of choices`.\n",
    "\n",
    "The following code allows us to define these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "\n",
    "param_sampling = GridParameterSampling( {\n",
    "        '--learning_rate': choice(3e-5, 3e-4),\n",
    "        '--num_epochs': choice(3, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to a define how we want to measure our performance. We do so by specifying two classes:\n",
    "\n",
    "- **[PrimaryMetricGoal](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.primarymetricgoal?view=azure-ml-py)**: We want to `MAXIMIZE` the `val_accuracy` that is logged in our training script.\n",
    "- **[BanditPolicy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py)**: A policy for early termination so that jobs which don't show promising results will stop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "\n",
    "primary_metric_name='val_accuracy'\n",
    "primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an estimator as usual, but this time without the script parameters that we are planning to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator4 = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--export_dir':'./outputs/model',\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add all our parameters in a [HyperDriveConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py) class and submit it as a run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator4,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name=primary_metric_name, \n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=10,\n",
    "                                         max_concurrent_runs=2)\n",
    "\n",
    "run4 = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we view the details of our run this time, we will see information and metrics for every run in our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the best run based on our defined metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run = run4.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "A registered [model](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model(class)?view=azure-ml-py) is a reference to the directory or file that make up your model. After registering a model, you and other people in your workspace can easily gain access to and deploy your model without having to run the training script again. \n",
    "\n",
    "We need to define the following parameters to register a model:\n",
    "\n",
    "- `model_name`: The name for your model. If the model name already exists in the workspace, it will create a new version for the model.\n",
    "- `model_path`: The path to where the model is stored. In our case, this was the *export_dir* defined in our estimators.\n",
    "- `description`: A description for the model.\n",
    "\n",
    "Let's register the best run from our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='azure-service-classifier', \n",
    "                                model_path='./outputs/model',\n",
    "                                datasets=[('train, test, validation data', azure_dataset)],\n",
    "                                description='BERT model for classifying azure services on stackoverflow posts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have registered the model with Dataset reference. \n",
    "* **ACTION**: Check dataset to model link in **Azure ML studio > Datasets tab > Azure Service Dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next tutorial](), we will perform inferencing on this model and deploy it to a web service."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
