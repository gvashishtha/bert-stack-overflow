{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 1: Download Stack Overflow data from archive. (This takes about 2-3 hours)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh \n",
    "wget https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z\n",
    "sudo apt-get install p7zip-full\n",
    "7z x stackoverflow.com-Posts.7z -oposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: Copy data over to databricks file system (This takes about an hour)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/databricks/driver/posts/Posts.xml\", \"dbfs:/tmp/posts/Posts.xml\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: Process data using Spark\n",
    "\n",
    "Note - This requires the spark-xml maven library (com.databricks:spark-xml_2.11:0.6.0) to be installed.\n",
    "'''\n",
    "import pyspark\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import size, col, concat_ws, rtrim, regexp_replace, split, udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# load xml file into spark data frame.\n",
    "posts = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(\"dbfs:/tmp/posts/Posts.xml\")\n",
    "\n",
    "# select only questions\n",
    "questions = posts.filter(posts._PostTypeId == 1) \n",
    "\n",
    "# drop irrelvant columns and clean up strings\n",
    "questions = questions.select([c for c in questions.columns if c in ['_Id','_Title','_Body','_Tags']])\n",
    "questions = questions.withColumn('full_question', sf.concat(sf.col('_Title'), sf.lit(' '), sf.col('_Body')))\n",
    "questions = questions.select([c for c in questions.columns if c in ['_Id','full_question','_Tags']]).withColumn(\"full_question\", regexp_replace(\"full_question\", \"[\\\\n,]\", \" \"))\n",
    "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"><\", \" \"))\n",
    "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"(>|<)\", \"\"))\n",
    "questions = questions.withColumn('_Tags', rtrim(questions._Tags))\n",
    "questions = questions.withColumn('_Tags', split(questions._Tags, \" \"))\n",
    "\n",
    "# filter out to single tags in following list\n",
    "tags_of_interest = ['azure-devops', 'azure-functions', 'azure-web-app-service', 'azure-storage', 'azure-virtual-machine'] \n",
    "\n",
    "def intersect(xs):\n",
    "    xs = set(xs)\n",
    "    @udf(\"array<string>\")\n",
    "    def _(ys):\n",
    "        return list(xs.intersection(ys))\n",
    "    return _\n",
    "\n",
    "questions = questions.withColumn(\"intersect\", intersect(tags_of_interest)(\"_Tags\"))\n",
    "questions = questions.filter(size(col(\"intersect\"))==1)\n",
    "questions = questions.select('_Id', 'full_question', 'intersect').withColumn('_Tags', concat_ws(', ', 'intersect'))\n",
    "questions = questions.select('_Id', 'full_question', '_Tags')\n",
    "\n",
    "questions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 4: Convert processed data into pandas data frame for final preprocessing and data split\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = questions.toPandas()\n",
    "\n",
    "# drop nan values and remove line breaks\n",
    "df.dropna(inplace=True)\n",
    "df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "\n",
    "# balance dataset \n",
    "balanced = df.groupby('_Tags')\n",
    "balanced.apply(lambda x: x.sample(balanced.size().min())).reset_index(drop=True).to_csv('balanced.csv')\n",
    "bd = pd.read_csv('balanced.csv')\n",
    "bd.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# shuffle data \n",
    "bd = shuffle(bd)\n",
    "\n",
    "# split data into train, test, and valid sets\n",
    "msk = np.random.rand(len(bd)) < 0.7\n",
    "train = bd[msk]\n",
    "temp = bd[~msk]\n",
    "msk = np.random.rand(len(temp)) < 0.66\n",
    "valid = temp[msk]\n",
    "test = temp[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: Save dataset into csv and class.txt files\n",
    "'''\n",
    "\n",
    "output_dir = './output'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create and save classes.txt file\n",
    "classes = pd.DataFrame(bd['_Tags'].unique().tolist())\n",
    "classes.to_csv(os.path.join(output_dir, 'classes.txt'), header=False, index=False)\n",
    "\n",
    "# save train, valid, and test files\n",
    "train.to_csv(os.path.join(output_dir, 'train.csv'), header=False, index=False)\n",
    "valid.to_csv(os.path.join(output_dir, 'valid.csv'), header=False, index=False)\n",
    "test.to_csv(os.path.join(output_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: Upload data to the default blob storage in your workspace.\n",
    "\n",
    "Note - This requires the azureml-sdk pip package to be installed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "\n",
    "subscription_id='<SUBSCRIPTION-ID>'\n",
    "resource_group='<RESOURCE-GROUP>'\n",
    "workspace_name='<WORKSPACE-NAME>'\n",
    "\n",
    "ws = Workspace(subscription_id=subscription_id, \n",
    "               resource_group=resource_group, \n",
    "               workspace_name=workspace_name)\n",
    "\n",
    "ds = Datastore.get_default(ws)\n",
    "ds.upload(src_dir='./output', target_path='test-service-classifier/data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "stackoverflow-data-prep",
  "notebookId": 4233017539218929
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
